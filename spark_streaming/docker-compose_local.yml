# version: '3'
# services:
#   spark-master:
#     image: bde2020/spark-master:3.0.0-hadoop3.2
#     container_name: spark-master
#     hostname: spark-master
#     healthcheck:
#       interval: 5s
#       retries: 100
#     environment:
#       - INIT_DAEMON_STEP=setup_spark
#     ports:
#       - "8080:8080"
#       - "7077:7077"
#     # volumes:
#     #   - ./spark-master:/tmp/spark-events
#     networks:
#       - project-network

#   spark-worker-1:
#     image: bde2020/spark-worker:3.0.0-hadoop3.2
#     container_name: spark-worker-1
#     hostname: spark-worker-1
#     depends_on:
#       - spark-master
#     environment:
#       - "SPARK_MASTER=spark://spark-master:7077"
#     expose:
#       - "8082"
#     ports:
#       - "8082:8082"
#     networks:
#       - project-network

#   spark-worker-2:
#     image: bde2020/spark-worker:3.0.0-hadoop3.2
#     container_name: spark-worker-2
#     hostname: spark-worker-2
#     depends_on:
#       - spark-master
#     environment:
#       - "SPARK_MASTER=spark://spark-master:7077"
#     expose:
#       - "8082"
#     ports:
#       - "8083:8083"
#     networks:
#       - project-network
# networks:
#   project-network:
#     external: true

version: '3'
services:
  spark-master:
    image: bde2020/spark-master:3.0.0-hadoop3.2
    container_name: spark-master
    hostname: spark-master
    healthcheck:
      interval: 5s
      retries: 100
    environment:
      - INIT_DAEMON_STEP=setup_spark
      - GOOGLE_APPLICATION_CREDENTIALS=/gcp-credentials/google_credentials.json
      # - HADOOP_CONF_DIR=/hadoop-conf
    volumes:
      - /media/longnd/New Volume/BigData/streamify/google_credentials.json:/gcp-credentials/google_credentials.json
      # - /path/to/your/hadoop/conf:/hadoop-conf
    ports:
      - "8080:8080"
      - "7077:7077"
    networks:
      - project-network

  spark-worker-1:
    image: bde2020/spark-worker:3.0.0-hadoop3.2
    container_name: spark-worker-1
    hostname: spark-worker-1
    depends_on:
      - spark-master
    environment:
      - "SPARK_MASTER=spark://spark-master:7077"
      - GOOGLE_APPLICATION_CREDENTIALS=/gcp-credentials/google_credentials.json
      # - HADOOP_CONF_DIR=/hadoop-conf
    expose:
      - "8082"
    ports:
      - "8082:8082"
    volumes:
      - /media/longnd/New Volume/BigData/streamify/google_credentials.json:/gcp-credentials/google_credentials.json
      # - /path/to/your/hadoop/conf:/hadoop-conf
    networks:
      - project-network

  spark-worker-2:
    image: bde2020/spark-worker:3.0.0-hadoop3.2
    container_name: spark-worker-2
    hostname: spark-worker-2
    depends_on:
      - spark-master
    environment:
      - "SPARK_MASTER=spark://spark-master:7077"
      - GOOGLE_APPLICATION_CREDENTIALS=/gcp-credentials/google_credentials.json
      # - HADOOP_CONF_DIR=/hadoop-conf
    expose:
      - "8082"
    ports:
      - "8083:8083"
    volumes:
      - /media/longnd/New Volume/BigData/streamify/google_credentials.json:/gcp-credentials/google_credentials.json
      # - /path/to/your/hadoop/conf:/hadoop-conf
    networks:
      - project-network

networks:
  project-network:
